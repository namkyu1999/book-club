# 메모리 타이밍

- 메인 메모리 → CPU에 비해 느려, 속도 향상 메커니즘의 필요
- 설계 계층
    - C 프로그래머
    - 컴파일러
    - 어셈블리 언어
    - CPU 명령어
    - 가상 메모리
    - 캐시 메모리의 여러 계층
    - 메인 메모리인 DRAM
- 데이터 센터 프로세스 → 많은 캐시를 내장하는 복잡한 칩을 가진다
- 서버 → 프로세서를 다수 포함

# 메모리

- 초기 메모리 → 메모리 비트당 하나의 구멍을 통과하는 와이어와 페라리트 코어로 구성
    - 코어를 통해 양의 전류 스파이크를 시계방향으로 통과 → 음의 전류 스파이크가 시계 반대 방향으로 흐른다.
    - 자력 → 전원 없이도 유지되어 하나의 비트를 기덕 가능(0/1 구분)
    - 메모리 → 바이트가 아닌 워드 단위 사용
        - 바이트 → 1964년으로, 이후에 도입됨.
    - 코어 메모리 → 메모리 워드 1비트에 와이어와 코어를 고밀도 배치

⇒ 메모리 읽기 → 평면의 수평과 수직 와이어에 전기를 흘려 코어를 0으로 만든 뒤, 나사형 감지 와이어를 통해 코어의 상태 변화로 파악

- 메모리 사이클 → 읽기와 복원을 하는 코어 메모리 사이클이 10마이크로sec → 1 마이크로sec으로 감소
- 페어차일드 반도체(존 슈미트) → 트랜지스터화된 정형 SRAM 발명
    - 비트당 6개의 트랜지스터, 속도는 빠르지만 비쌈
    - 자기상태가 없어 전원이 꺼지면 사라짐
- 로버트 대널드 → .DRAM 발명
    - 비트당 하나의 트랜지스터만 사용, SRAM보다 밀도가 높아 저렴하지만 속도가 느리다.
- DRAM → 읽기도 마찬가지로 읽은 비트를 잃어버린다는 특징 O
    - 읽을 때 마다 비트당 하나씩 저장된 트랜지스터의 전하 소모 → 읽은 비트는 트랜지스터 기록

# 캐시 구조

여러 계층의 캐시 메모리와 거대한 메인 메모리로 구성된 계층 구조를 가짐.

- L1 계층 명령어 캐시 + 데이터 캐시 → 물리적 CPU 코어당 하나씩 L2에서 채워진다.
- L2캐시는 L1캐시와 데이터 캐시보다 크기는 크지만 속도는 느리며, CPU 코어간 공유되기도 한다.
- 큰 캐시 계층 → 속도가 충분히 빠르고 공간이 커서 코어간 공유되면 좋지만, 반도체 메모리 접근 시간은 크기가 커질수록 비례해서 늘어나 불가능
    - 최신 캐시 설계 → 매모리 접근 패턴이 동일한 위치에서 접근하는 경우 낮은 비요ㅗㅇ으로 고성능 제공.
- L1 캐시 → CPU 사이클마다 한두개의 낮은 지연시간 접근
    - 메인 메모리 시스템의 각 뱅크 → 50~100 CPU 사이클마다 새로운 접근 불가
    - 캐시 계층에 데이터와 명령어 배치 → CPU 성능에 영향 ㅒ
- 캐시 메모리 → 몇 바이트의 라인 혹은 블록으로 구성
    - 라인의 모든 바이트 → 함께 꺼내 추가됨.
    - 캐시 히트 → 캐시에서 찾은 데이터가 하드웨어 메모리 주소와 맞는 것, 아니라면 Cache miss가 되며, 메모리 계층의 하위 계층으로 내려간다.

### 완전 연관 사상 캐시

메인 메모리 주소가 있는 데이터 → 캐시 라인에 저장

- 데이터 찾기 → 메모리 주소를 캐시의 모든 태그와 비교
    - 하나의 태그 일치 → 해당 라인의 데이터에는 빠르게 접근 가능
    - 일치 X → 데이터는 느린 메모리에 접근 뒤 해당 데이터를 채워 다시 접근한다면 빠르게 찾도록 일부 캐시 라인 비교
    - 둘 이상의 태그 일치 → 심각한 버그 !

### 직접 사상 캐시

메인 메모리 주소 데이터를 정확히 하나의 캐시 라인에 저장 → 보통 낮은 메모리 주소 비트로 캐시 라인을 선택해 최대한 분산

- 자주 접근하는 두개의 데이터가 동일한 캐시 라인에 위치할때마다 성능 저하
    - 한 항목에 대한 접근 → 다른 캐시의 항목 제거
    - 번갈아 접근 → 캐시미스의 100% 발생으로 메모리가 느려짐

### 직접 연관 사상 캐시

위 둘의 중간 지점이 되는 캐시

- 메인 메모리 주소가 있는 데이터 → N 웨이 연관 사상 캐시에서, 정확히 하나의 N 캐시 라인에 저장
- 각 세트의 메모리 주소 → N개의 태그와 비교
    - N은 2~16 사이의 값
- 4 way 집합 연관 사상 캐시 → 4개의 캐시 라인 포함

# 데이터 정렬

- 4 바이트 항목에 대한 정렬된 참조 → 4의 배수인 바이트 주소로 구성
- 8 바이트 항목에 대한 정렬된 참조 → 8의 배수인 바이트 주소로 구성

⇒ 실제 캐시와 메모리 접근은 정렬된 양만큼만 수행

- 정렬되지 않은 참조 → 정렬된 두 위치에 접근한 뒤 바이트 이동과 병합한 후 접근

# 변환 색인 버퍼 구조

- 가상 주소를 물리 주소로 변환하는 정보 저장 → 변환 색인 버퍼(TLB) 구조 사용
    - TLB는 CPU 코어당 코어 내부나 여러 코어끼리 공유되는 1계층 TLB
    - 더 크지만 느린 2계층 TLB가 존재
    
    ⇒ 매커니즘 속도는 증가시키지만, CPU 성능에 복잡성과 가변성을 더한다.
    
- TLB → 캐시와 상호작용 진행
    - 캐시 → 물리 주소로 세트 선택과 태그값은 가상 주소에서 물리 메모리 주소로 변환해 가져온다.
    - 맵핑되지 않은 가상주소로 세트를 선택하고 태그값을 사용하는 설꼐 또한 가능
        - 두개의 가상주소가 같은 물리주소에 매핑되는 경우, 서로 다른 캐시라인에 가상 주소가 매핑되는 문제 발생
        - 중복 매핑 → 데이터가 읽기전용, 명령어 스트림인 경우에는 문제 X, But 하나의 가상주소에 작성된 후 읽는경우 문제가 됨
    - 물리주소의 지정된 캐시가 한 페이지보다 큰 경우 → 캐시 태그를 비교하기 전 가상 주소와 물리 주소의 매핑
        - CPU에서 캐시 된 데이터에 접근하기 전 태그 비교 완료 필요
        
        ⇒ L1 캐시 접근이 가장 빠른 하드웨어가 사용되므로 많은 양의 전력과 칩공간 필요
        
        - **속도를 높이려면, 가상 주소를 물리주소와 맵핑이 완료되기 전 맵핑되지 않은 하위 가상 주소 비트를 이용해 올바른 캐시 선택 필요**

# 캐시 라인 크기 측정

- 데이터가 전혀 들어있지 않은 캐시에서 X에 정렬된 단어를로드한 뒤 X+c에 단어를 로드하는 것(C는 가능한 캐시라인의 크기)
- X가 라인경계, c가 라인 크기보다 작은 경우 → X+c를 로드 시 캐시 히트 발생

# N+1 프리패칭

- X, X+c, X+2c등 일정하게 증가하는 스트라이드 형태 → 단순
    - 현대의 캐시는 N번 라인에 접근 시 N+1 라인을 프리패치 하는 방법으로, 나머지 캐시미스를 제거
    - 현대 CPU는 비순차 실행으로, 사이클마다 여러 명령어를 실행해 메모리를 응답하도록 5~50개의 끝나지 않은 로드를 병렬로 대기시킨다
    
    ⇒ 캐시 미스가 발생하는 시간이 단 한번의 캐시 미스 시간과 같아, 전체 캐시 미스 시간을 10배 이상 줄이는 것이 가능하다.
    
- 일정하게 증가하는 스트라이드의 접근 형태 구현 → 너무 빠르고 균일하게 잘못된 결과 도출
    - 한쌍씩 떨어진 16바이트 항목에 접근 → 사이사이 40MB의 관련 없는 데이터 로드 → 캐시를 지우며 간격 변경
        - 40MB → 최대 예상 사이즈인 32MB보다 다소 큰값이기 때문

# 종속적인 로드

- 메모리의 연결 리스트로 관리 및, 캐시 라인 크기에 맞춘 항목을 캐시 라인 경계에 맞추어 구성
- 프리패치를 피하려면 주소공간에서 뒤섞이도록 연결하면 된다.

# 무작위가 아닌 DRAM

캐시미스 → 수백개의 메모리 로드 타이밍으로 계산, 두배이상의 캐시 미스 발생시 두배 더 오래 걸린다.

- DRAM → 막상 랜덤하게 동작 X, 내부 큰 비트 열의 행에 접근한 뒤, 그 행의 바이트 열에 접근
    - DRAM에 대한 두개의 잇따르는 접근이 두개의 다른행에 있을때의 접근 순서
        - 프리차지 → 행접근  → 열접근
    
    ⇒ 두번째 접근이 첫번째와 동일한 행이면 CPU와 DRAM 하드웨어는 숏컷을 이용해 3배 더 빠르다.
    

# 캐시 계층별 크기 측정

N 바이트를 캐시에서 읽은 뒤, 타이밍을 보고 다시 읽기

- 캐시에 모든 N바이트가 들어가면 읽기 속도가 빨라진다.
- 특정 바이트가 맞지 않을 때 → 부분이나 전체 다시읽기가 누락되고, 다음 계층으로 내려가 채워진다.

**⇒ But 실제 장비와 운영체제의 타이밍이 정확히 반복되는 경우는 적다, 타이밍 데이터는 약간의 노이즈가 있어 타이밍 계산은 쉽지 않다.**

- N ≤ L1 캐시 전체 크기면, 재읽기 속도는 로드당 단지 몇 사이클이므로 L1 접근시간만큼 빠를 수 있다
- L1 ≤ N ≤ L2면 다소 느린 L2 접근시간 정도로 볼 수 있다.

# 캐시 계층별 접근도 측정

- 완전 연관 사상 캐시 → 캐시 라인은 캐시의 어느곳이든 위치 가능
- 집합 연관 사상 캐시 → 캐시 라인이 한 세트 내 적은 수의 공간에만 위치 가능
- N way 연관 사상 캐시 → 일반적으로 N개의 태그와 데이터 위치에 병렬 접근 진행
    - 한 위치에 접근하는 것에 비해 N배의 전력 사용
    - 태그중 하나가 지정한 주소와 일치하면 캐시 히트, 해당 데이터 사용 후 일치하는 항목이 없으면 미스 발생

# 변환 버퍼 시간

현대 프로세서 → 메모리에 접근하며 동시에 가상 메모리 페이지에도 접근, 수십 MB의 데이터를 읽는다면 CPU 코어의 하드웨어 TLB도 제거하는 것이 가능해진다.

- 메모리 접근 중 일부 → 해당하는 TLB 엔트리를 로드하려면, 페이지 테이블 메모리 접근을 먼저 수행해야 해서 2배의 접근 필요

# 활용도 낮은 캐시

- 메모리 주소의 가장 낮은 비트 → 캐시 라인에서 사용되는 바이트를 선택하는데 사용
    - 라인의 크기가 64바이트 → 가장 낮은 6비트
        - 그다음 높은 비트 → 연관집합을 선택하는데 사용된다.

⇒ 64바이트 라인 캐시 데이터 로드 시 128바이트 배수이므로, 절반은 사용되지 않아 느린 실행 속도를 의미하게 된다.